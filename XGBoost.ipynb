{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada1910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import ydata_profiling\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74ce3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel(r'C:\\Users\\sg_cl\\Desktop\\masters subjects\\DSRT 736\\Fraud Dataset\\New_data_CCFD\\ccfraudTrain.xlsx')\n",
    "\n",
    "df1 = pd.read_excel(r'C:\\Users\\sg_cl\\Desktop\\masters subjects\\DSRT 736\\Fraud Dataset\\New_data_CCFD\\ccfraudTest.xlsx')\n",
    "# For snakey diagram\n",
    "#df1 = pd.read_excel(r'C:\\Users\\sg_cl\\Desktop\\masters subjects\\DSRT 736\\Fraud Dataset\\New_data_CCFD\\Sankey_data.xlsx')mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e678e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# checking the dataset\n",
    "df.info()\n",
    "df.describe()\n",
    "df1.info()\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8c636f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print (df['trans_date_trans_time'].head())\n",
    "from datetime import datetime as dt\n",
    "import pytz  # Import the pytz module for time zones\n",
    "\n",
    "df['trans_date_trans_time_HR'] = df['trans_date_trans_time'].dt.strftime(\"%H\")\n",
    "\n",
    "# Convert the Unix time values to datetime objects\n",
    "df['unix_time_NEW'] = df['unix_time'].apply(lambda x: dt.fromtimestamp(x))\n",
    "\n",
    "# Set the time zone to the desired time zone (e.g., 'UTC' or your local time zone)\n",
    "local_time_zone = 'America/New_York'\n",
    "df['unix_time_NEW'] = df['unix_time_NEW'].dt.tz_localize(pytz.utc).dt.tz_convert(local_time_zone)\n",
    "\n",
    "\n",
    "print (df.head())\n",
    "\n",
    "df1['trans_date_trans_time_HR'] = df1['trans_date_trans_time'].dt.strftime(\"%H\")\n",
    "\n",
    "# Convert the Unix time values to datetime objects\n",
    "df1['unix_time_NEW'] = df1['unix_time'].apply(lambda x: dt.fromtimestamp(x))\n",
    "\n",
    "# Set the time zone to the desired time zone (e.g., 'UTC' or your local time zone)\n",
    "local_time_zone = 'America/New_York'\n",
    "df1['unix_time_NEW'] = df1['unix_time_NEW'].dt.tz_localize(pytz.utc).dt.tz_convert(local_time_zone)\n",
    "\n",
    "\n",
    "print (df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1f030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('unix_time', axis=1)\n",
    "df = df.drop('unix_time_NEW', axis=1)\n",
    "df1 = df1.drop('unix_time', axis=1)\n",
    "df1 = df1.drop('unix_time_NEW', axis=1)\n",
    "#df = df.drop('Errors?', axis=1)\n",
    "#df = df.drop('Time', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b43a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965c904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.corr())\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ce5a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = pandas_profiling.ProfileReport(df)\n",
    "df['trans_date_trans_time_HR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf90d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile.to_file(\"data_profile_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4400ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset 1\n",
    "# Convert 'Timestamp' column to datetime type\n",
    "#main_df['Timestamp'] = pd.to_datetime(main_df['Timestamp'])\n",
    "\n",
    "# Select the desired columns and perform aggregation on the subset\n",
    "subset_df = df[[ 'cc_num', 'trans_date_trans_time_HR', 'merchant', 'category', 'gender','state','lat', 'long','merch_lat',\n",
    "       'merch_long', 'amt','is_fraud']].reset_index()\n",
    "subset_df1 = df1[[ 'cc_num', 'trans_date_trans_time_HR', 'merchant', 'category', 'gender','state','lat', 'long','merch_lat',\n",
    "       'merch_long', 'amt','is_fraud']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3c6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "subset_df = pd.get_dummies(subset_df, columns=['merchant', 'category', 'gender', 'state'])\n",
    "\n",
    "subset_df1 = pd.get_dummies(subset_df1, columns=['merchant', 'category', 'gender', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaacd48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_df.head()\n",
    "subset_df = subset_df.drop('state_DE', axis=1)\n",
    "#subset_df1.head()\n",
    "#subset_df1['state_DE'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd38b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data pre processing String values to Int values\n",
    "print(df['Is Fraud?'].unique())\n",
    "y = df['Is Fraud?']\n",
    "enc = preprocessing.LabelEncoder()\n",
    "enc.fit(y)\n",
    "y = enc.transform(y)\n",
    "print(y)\n",
    "\n",
    "\n",
    "# Separate features (X) and labels (y)\n",
    "X = df.drop('Is Fraud?', axis=1)\n",
    "print(X.head())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec48a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = subset_df.drop('is_fraud', axis=1)\n",
    "y = subset_df['is_fraud']\n",
    "\n",
    "X1 = subset_df1.drop('is_fraud', axis=1)\n",
    "y1 = subset_df1['is_fraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f7edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X_train,X_test, y_train,y_test = train_test_split(X, y,test_size=0.2, stratify=y, random_state=42)\n",
    "#X_val,X_test,y_val, y_test = train_test_split(X1, y1,test_size=0.5, stratify=y1, random_state=42)\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the resulting subsets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946331dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "# = LinearRegression()\n",
    "# Define class weights (adjust these based on your problem)\n",
    "genuine_weight = 1.0 #6006 / (1042569 + 6006)\n",
    "fraud_weight = 9.0 #1042569 / (1042569 + 6006)\n",
    "\n",
    "class_weights = {0: genuine_weight, 1: fraud_weight}\n",
    "#class_weights = {0: 1.0, 1: 10.0}\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model1 = RandomForestClassifier(n_estimators=30,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model2 = RandomForestClassifier(n_estimators=70,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model3 = RandomForestClassifier(n_estimators=80,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model4 = RandomForestClassifier(n_estimators=90,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca093076",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0820e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = 'is_fraud'\n",
    "X_train = subset_df.drop(columns = [cols_to_exclude])\n",
    "y_train = subset_df[['is_fraud']]\n",
    "X_test = subset_df1.drop(columns = [cols_to_exclude])\n",
    "y_test = subset_df1[['is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Train the model\n",
    "model1.fit(X_train, y_train)\n",
    "# Train the model\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "model4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a689cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b731ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y1_pred = model1.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y2_pred = model2.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y3_pred = model3.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y4_pred = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c281ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y1_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y1_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y2_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y2_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y3_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y3_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y4_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c13e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace clf with your Random Forest model and X_test with your test data\n",
    "predicted_probabilities = model.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities1 = model1.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities2 = model2.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities3 = model3.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities4 = model4.predict_proba(X_test)[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c17eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c275387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y_pred = (predicted_probabilities >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y1_pred = (predicted_probabilities1 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y1_pred)\n",
    "    recall = recall_score(y_test, y1_pred)\n",
    "    f1 = f1_score(y_test, y1_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaafdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y2_pred = (predicted_probabilities2 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y2_pred)\n",
    "    recall = recall_score(y_test, y2_pred)\n",
    "    f1 = f1_score(y_test, y2_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfe942",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y3_pred = (predicted_probabilities3 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y3_pred)\n",
    "    recall = recall_score(y_test, y3_pred)\n",
    "    f1 = f1_score(y_test, y3_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cbe34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "thresholds = [0.2] \n",
    "#thresholds = [0.2,0.3, 0.4, 0.5, 0.6, 0.7,0.8]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y4_pred = (predicted_probabilities4 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y4_pred)\n",
    "    recall = recall_score(y_test, y4_pred)\n",
    "    f1 = f1_score(y_test, y4_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y4_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "#X_train,X_test, y_train,y_test = train_test_split(X, y,test_size=0.2, stratify=y, random_state=42)\n",
    "X_val,X_test,y_val, y_test = train_test_split(X1, y1,test_size=0.5, stratify=y1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y4_pred = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23864a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3] \n",
    "#thresholds = [0.2,0.3, 0.4, 0.5, 0.6, 0.7,0.8]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y4_pred = (predicted_probabilities4 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y4_pred)\n",
    "    recall = recall_score(y_test, y4_pred)\n",
    "    f1 = f1_score(y_test, y4_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y4_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, predicted_probabilities4)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f58ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predicted labels to the dataset\n",
    "X_test['predicted_label'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264da57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_totals = df.groupby('is_fraud')['is_fraud'].count()\n",
    "print(category_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify false negative transactions\n",
    "false_negatives = X_test[(X_test['predicted_label'] == 0) & (X_test['is_fraud'] == 1)]\n",
    "print(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "# Set a threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# Convert predictions to binary classes based on the threshold\n",
    "y_pred_binary = [1 if val >= threshold else 0 for val in y_pred]\n",
    "\n",
    "# Print the binary predictions\n",
    "print(\"Binary Predictions:\", y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74955d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example for validation (optional)\n",
    "example_index = 208\n",
    "example_features = X.iloc[example_index]\n",
    "example_target = y[example_index]\n",
    "\n",
    "# Make predictions on the example\n",
    "example_prediction = model.predict([example_features])\n",
    "\n",
    "# Compare with actual value\n",
    "print(\"Example features:\", example_features)\n",
    "print(\"Example target:\", example_target)\n",
    "print(\"Example prediction:\", example_prediction)\n",
    "\n",
    "# Calculate mean squared error on the test set (optional)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_pred_binary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d3e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
