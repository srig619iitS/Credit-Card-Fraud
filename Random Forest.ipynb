{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada1910b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import ydata_profiling\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e74ce3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_excel(r'C:\\Users\\sg_cl\\Desktop\\masters subjects\\DSRT 736\\Fraud Dataset\\New_data_CCFD\\ccfraudTrain.xlsx')\n",
    "\n",
    "df1 = pd.read_excel(r'C:\\Users\\sg_cl\\Desktop\\masters subjects\\DSRT 736\\Fraud Dataset\\New_data_CCFD\\ccfraudTest.xlsx')\n",
    "# For snakey diagram\n",
    "#df1 = pd.read_excel(r'C:\\Users\\sg_cl\\Desktop\\masters subjects\\DSRT 736\\Fraud Dataset\\New_data_CCFD\\Sankey_data.xlsx')mk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e678e1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'trans_date_trans_time', 'cc_num', 'merchant', 'category',\n",
       "       'amt', 'first', 'last', 'gender', 'street', 'city', 'state', 'zip',\n",
       "       'lat', 'long', 'city_pop', 'job', 'dob', 'trans_num', 'unix_time',\n",
       "       'merch_lat', 'merch_long', 'is_fraud'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec9fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# checking the dataset\n",
    "df.info()\n",
    "df.describe()\n",
    "df1.info()\n",
    "df1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c8c636f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2019-01-01 00:00:18  2703186189652090   \n",
      "1           1   2019-01-01 00:00:44      630423337322   \n",
      "2           2   2019-01-01 00:00:51    38859492057661   \n",
      "3           3   2019-01-01 00:01:16  3534093764340240   \n",
      "4           4   2019-01-01 00:03:06   375534208663984   \n",
      "\n",
      "                             merchant       category     amt      first  \\\n",
      "0          fraud_Rippin, Kub and Mann       misc_net    4.97   Jennifer   \n",
      "1     fraud_Heller, Gutmann and Zieme    grocery_pos  107.23  Stephanie   \n",
      "2                fraud_Lind-Buckridge  entertainment  220.11     Edward   \n",
      "3  fraud_Kutch, Hermiston and Farrell  gas_transport   45.00     Jeremy   \n",
      "4                 fraud_Keeling-Crist       misc_pos   41.96      Tyler   \n",
      "\n",
      "      last gender                        street  ... city_pop  \\\n",
      "0    Banks      F                561 Perry Cove  ...     3495   \n",
      "1     Gill      F  43039 Riley Greens Suite 393  ...      149   \n",
      "2  Sanchez      M      594 White Dale Suite 530  ...     4154   \n",
      "3    White      M   9443 Cynthia Court Apt. 038  ...     1939   \n",
      "4   Garcia      M              408 Bradley Rest  ...       99   \n",
      "\n",
      "                                 job        dob  \\\n",
      "0          Psychologist, counselling 1988-03-09   \n",
      "1  Special educational needs teacher 1978-06-21   \n",
      "2        Nature conservation officer 1962-01-19   \n",
      "3                    Patent attorney 1967-01-12   \n",
      "4     Dance movement psychotherapist 1986-03-28   \n",
      "\n",
      "                          trans_num   unix_time  merch_lat  merch_long  \\\n",
      "0  0b242abb623afc578575680df30655b9  1325376018  36.011293  -82.048315   \n",
      "1  1f76529f8574734946361c461b024d99  1325376044  49.159047 -118.186462   \n",
      "2  a1a22d70485983eac12b5b88dad1cf95  1325376051  43.150704 -112.154481   \n",
      "3  6b849c168bdad6f867558c3793159a81  1325376076  47.034331 -112.561071   \n",
      "4  a41d7549acf90789359a9aa5346dcb46  1325376186  38.674999  -78.632459   \n",
      "\n",
      "  is_fraud trans_date_trans_time_HR             unix_time_NEW  \n",
      "0        0                       00 2011-12-31 14:00:18-05:00  \n",
      "1        0                       00 2011-12-31 14:00:44-05:00  \n",
      "2        0                       00 2011-12-31 14:00:51-05:00  \n",
      "3        0                       00 2011-12-31 14:01:16-05:00  \n",
      "4        0                       00 2011-12-31 14:03:06-05:00  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "   Unnamed: 0 trans_date_trans_time            cc_num  \\\n",
      "0           0   2020-06-21 12:14:25  2291163933867240   \n",
      "1           1   2020-06-21 12:14:33  3573030041201290   \n",
      "2           2   2020-06-21 12:14:53  3598215285024750   \n",
      "3           3   2020-06-21 12:15:15  3591919803438420   \n",
      "4           4   2020-06-21 12:15:17  3526826139003040   \n",
      "\n",
      "                               merchant        category    amt   first  \\\n",
      "0                 fraud_Kirlin and Sons   personal_care   2.86    Jeff   \n",
      "1                  fraud_Sporer-Keebler   personal_care  29.84  Joanne   \n",
      "2  fraud_Swaniawski, Nitzsche and Welch  health_fitness  41.28  Ashley   \n",
      "3                     fraud_Haley Group        misc_pos  60.05   Brian   \n",
      "4                 fraud_Johnston-Casper          travel   3.19  Nathan   \n",
      "\n",
      "       last gender                       street  ... city_pop  \\\n",
      "0   Elliott      M            351 Darlene Green  ...   333497   \n",
      "1  Williams      F             3638 Marsh Union  ...      302   \n",
      "2     Lopez      F         9333 Valentine Point  ...    34496   \n",
      "3  Williams      M  32941 Krystal Mill Apt. 552  ...    54767   \n",
      "4    Massey      M     5783 Evan Roads Apt. 465  ...     1126   \n",
      "\n",
      "                      job        dob                         trans_num  \\\n",
      "0     Mechanical engineer 1968-03-19  2da90c7d74bd46a0caf3777415b3ebd3   \n",
      "1  Sales professional, IT 1990-01-17  324cc204407e99f51b0d6ca0055005e7   \n",
      "2       Librarian, public 1970-10-21  c81755dbbbea9d5c77f094348a7579be   \n",
      "3            Set designer 1987-07-25  2159175b9efe66dc301f149d3d5abf8c   \n",
      "4      Furniture designer 1955-07-06  57ff021bd3f328f8738bb535c302a31b   \n",
      "\n",
      "    unix_time  merch_lat  merch_long is_fraud trans_date_trans_time_HR  \\\n",
      "0  1371816865  33.986391  -81.200714        0                       12   \n",
      "1  1371816873  39.450498 -109.960431        0                       12   \n",
      "2  1371816893  40.495810  -74.196111        0                       12   \n",
      "3  1371816915  28.812398  -80.883061        0                       12   \n",
      "4  1371816917  44.959148  -85.884734        0                       12   \n",
      "\n",
      "              unix_time_NEW  \n",
      "0 2013-06-21 04:14:25-04:00  \n",
      "1 2013-06-21 04:14:33-04:00  \n",
      "2 2013-06-21 04:14:53-04:00  \n",
      "3 2013-06-21 04:15:15-04:00  \n",
      "4 2013-06-21 04:15:17-04:00  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "#print (df['trans_date_trans_time'].head())\n",
    "from datetime import datetime as dt\n",
    "import pytz  # Import the pytz module for time zones\n",
    "\n",
    "df['trans_date_trans_time_HR'] = df['trans_date_trans_time'].dt.strftime(\"%H\")\n",
    "\n",
    "# Convert the Unix time values to datetime objects\n",
    "df['unix_time_NEW'] = df['unix_time'].apply(lambda x: dt.fromtimestamp(x))\n",
    "\n",
    "# Set the time zone to the desired time zone (e.g., 'UTC' or your local time zone)\n",
    "local_time_zone = 'America/New_York'\n",
    "df['unix_time_NEW'] = df['unix_time_NEW'].dt.tz_localize(pytz.utc).dt.tz_convert(local_time_zone)\n",
    "\n",
    "\n",
    "print (df.head())\n",
    "\n",
    "df1['trans_date_trans_time_HR'] = df1['trans_date_trans_time'].dt.strftime(\"%H\")\n",
    "\n",
    "# Convert the Unix time values to datetime objects\n",
    "df1['unix_time_NEW'] = df1['unix_time'].apply(lambda x: dt.fromtimestamp(x))\n",
    "\n",
    "# Set the time zone to the desired time zone (e.g., 'UTC' or your local time zone)\n",
    "local_time_zone = 'America/New_York'\n",
    "df1['unix_time_NEW'] = df1['unix_time_NEW'].dt.tz_localize(pytz.utc).dt.tz_convert(local_time_zone)\n",
    "\n",
    "\n",
    "print (df1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6d1f030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.drop('unix_time', axis=1)\n",
    "df = df.drop('unix_time_NEW', axis=1)\n",
    "df1 = df1.drop('unix_time', axis=1)\n",
    "df1 = df1.drop('unix_time_NEW', axis=1)\n",
    "#df = df.drop('Errors?', axis=1)\n",
    "#df = df.drop('Time', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94b43a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965c904",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df.corr())\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf90d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "profile.to_file(\"data_profile_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681704a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4400ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset 1\n",
    "# Convert 'Timestamp' column to datetime type\n",
    "#main_df['Timestamp'] = pd.to_datetime(main_df['Timestamp'])\n",
    "\n",
    "# Select the desired columns and perform aggregation on the subset\n",
    "subset_df = df[[ 'cc_num', 'trans_date_trans_time_HR', 'merchant', 'category', 'gender','state','lat', 'long','merch_lat',\n",
    "       'merch_long', 'amt','is_fraud']].reset_index()\n",
    "subset_df1 = df1[[ 'cc_num', 'trans_date_trans_time_HR', 'merchant', 'category', 'gender','state','lat', 'long','merch_lat',\n",
    "       'merch_long', 'amt','is_fraud']].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa3c6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "subset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5fe9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to numerical using one-hot encoding\n",
    "subset_df = pd.get_dummies(subset_df, columns=['merchant', 'category', 'gender', 'state'])\n",
    "\n",
    "subset_df1 = pd.get_dummies(subset_df1, columns=['merchant', 'category', 'gender', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaacd48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#subset_df.head()\n",
    "subset_df = subset_df.drop('state_DE', axis=1)\n",
    "#subset_df1.head()\n",
    "#subset_df1['state_DE'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bec48a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features (X) and target (y)\n",
    "X = subset_df.drop('is_fraud', axis=1)\n",
    "y = subset_df['is_fraud']\n",
    "\n",
    "X1 = subset_df1.drop('is_fraud', axis=1)\n",
    "y1 = subset_df1['is_fraud']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2cb85943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    553574\n",
      "1      2145\n",
      "Name: is_fraud, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get value counts\n",
    "#value_counts = df1['is_fraud'].value_counts()\n",
    "#value_counts = subset_df1['is_fraud'].value_counts()\n",
    "value_counts = y1.value_counts()\n",
    "\n",
    "\n",
    "# Print value counts\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f7edb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Missing values:\", df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77154d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of test sizes to experiment with\n",
    "test_sizes = np.linspace(0.1, 0.5, 5)  # Adjust the range as needed\n",
    "# Define a range of n_estimators to experiment with\n",
    "n_estimators_values = range(10, 131, 10)\n",
    "\n",
    "# Define class weights\n",
    "class_weights = {0: 1.0, 1: 4.0}\n",
    "\n",
    "# Initialize an empty list to store cross-validation scores\n",
    "results = []\n",
    "\n",
    "# Loop through different test sizes\n",
    "for test_size in test_sizes:\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    # Initialize and train your model (for example, RandomForestClassifier, Parallel execution)\n",
    "    model = RandomForestClassifier(random_state=42 , n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Perform cross-validation on the training set\n",
    "    cv_score = np.mean(cross_val_score(model, X_train, y_train, cv=5))  # 5-fold cross-validation\n",
    "    \n",
    "    # Store the cross-validation score\n",
    "    cv_scores.append(cv_score)\n",
    "\n",
    "# Find the test size with the highest cross-validation score\n",
    "best_test_size = test_sizes[np.argmax(cv_scores)]\n",
    "\n",
    "print(f\"The optimum test size is: {best_test_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0db44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_size in test_sizes:\n",
    "    for n_estimators in n_estimators_values:\n",
    "        # Split the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "        \n",
    "        # Initialize and train your model (for example, RandomForestClassifier)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Perform cross-validation on the training set\n",
    "        cv_score = np.mean(cross_val_score(model, X_train, y_train, cv=5))  # 5-fold cross-validation\n",
    "        \n",
    "        # Store the results\n",
    "        results.append((test_size, n_estimators, cv_score))\n",
    "\n",
    "# Find the best combination of test size and n_estimators\n",
    "best_result = max(results, key=lambda x: x[2])\n",
    "best_test_size, best_n_estimators, best_cv_score = best_result\n",
    "\n",
    "print(f\"The optimum test size is: {best_test_size}\")\n",
    "print(f\"The optimum n_estimators is: {best_n_estimators}\")\n",
    "print(f\"The corresponding cross-validation score is: {best_cv_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49e89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X_train,X_test, y_train,y_test = train_test_split(X, y,test_size=0.2, stratify=y, random_state=42)\n",
    "#X_val,X_test,y_val, y_test = train_test_split(X1, y1,test_size=0.5, stratify=y1, random_state=42)\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, stratify=y_test, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of the resulting subsets\n",
    "print(\"Training set shape:\", X_train.shape, y_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946331dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression model\n",
    "# = LinearRegression()\n",
    "# Define class weights (adjust these based on your problem)\n",
    "genuine_weight = 1.0 #6006 / (1042569 + 6006)\n",
    "fraud_weight = 4.0 #1042569 / (1042569 + 6006)\n",
    "\n",
    "class_weights = {0: genuine_weight, 1: fraud_weight}\n",
    "#class_weights = {0: 1.0, 1: 10.0}\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model1 = RandomForestClassifier(n_estimators=30,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model2 = RandomForestClassifier(n_estimators=70,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model3 = RandomForestClassifier(n_estimators=80,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)\n",
    "model4 = RandomForestClassifier(n_estimators=90,class_weight=class_weights, random_state=42,n_jobs=-1,max_samples=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca093076",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0820e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_exclude = 'is_fraud'\n",
    "X_train = subset_df.drop(columns = [cols_to_exclude])\n",
    "y_train = subset_df[['is_fraud']]\n",
    "X_test = subset_df1.drop(columns = [cols_to_exclude])\n",
    "y_test = subset_df1[['is_fraud']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffb4729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Train the model\n",
    "model1.fit(X_train, y_train)\n",
    "# Train the model\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "model4.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b731ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y1_pred = model1.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y2_pred = model2.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y3_pred = model3.predict(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y4_pred = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c281ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y1_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y1_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y2_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y2_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y3_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y3_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y4_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c13e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Replace clf with your Random Forest model and X_test with your test data\n",
    "predicted_probabilities = model.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities1 = model1.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities2 = model2.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities3 = model3.predict_proba(X_test)[:, 1] \n",
    "predicted_probabilities4 = model4.predict_proba(X_test)[:, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c17eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c275387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y_pred = (predicted_probabilities >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bcba52",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y1_pred = (predicted_probabilities1 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y1_pred)\n",
    "    recall = recall_score(y_test, y1_pred)\n",
    "    f1 = f1_score(y_test, y1_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaafdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y2_pred = (predicted_probabilities2 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y2_pred)\n",
    "    recall = recall_score(y_test, y2_pred)\n",
    "    f1 = f1_score(y_test, y2_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedfe942",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.4, 0.5, 0.6, 0.7]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y3_pred = (predicted_probabilities3 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y3_pred)\n",
    "    recall = recall_score(y_test, y3_pred)\n",
    "    f1 = f1_score(y_test, y3_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9cbe34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#thresholds = [0.2] \n",
    "thresholds = [0.2,0.3, 0.4, 0.5, 0.6, 0.7,0.8]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y4_pred = (predicted_probabilities4 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y4_pred)\n",
    "    recall = recall_score(y_test, y4_pred)\n",
    "    f1 = f1_score(y_test, y4_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")\n",
    "    #print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y4_pred))\n",
    "    #print(\"\\nClassification Report:\\n\", classification_report(y_test, y4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918d1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "#X_train,X_test, y_train,y_test = train_test_split(X, y,test_size=0.2, stratify=y, random_state=42)\n",
    "X_val,X_test,y_val, y_test = train_test_split(X1, y1,test_size=0.5, stratify=y1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y4_pred = model4.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0504c426",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23864a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3] \n",
    "#thresholds = [0.2,0.3, 0.4, 0.5, 0.6, 0.7,0.8]  # Example threshold values\n",
    "for threshold in thresholds:\n",
    "    y4_pred = (predicted_probabilities4 >= threshold).astype(int)\n",
    "    precision = precision_score(y_test, y4_pred)\n",
    "    recall = recall_score(y_test, y4_pred)\n",
    "    f1 = f1_score(y_test, y4_pred)\n",
    "    print(f\"Threshold: {threshold:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f} | F1-score: {f1:.2f}\")\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y4_pred))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y4_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8013e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, predicted_probabilities4)\n",
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f58ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the predicted labels to the dataset\n",
    "X_test['predicted_label'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c1c278",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264da57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_totals = df.groupby('is_fraud')['is_fraud'].count()\n",
    "print(category_totals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bba8360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8dd432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify false negative transactions\n",
    "false_negatives = X_test[(X_test['predicted_label'] == 0) & (X_test['is_fraud'] == 1)]\n",
    "print(false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dd673",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred)\n",
    "# Set a threshold\n",
    "threshold = 0.01\n",
    "\n",
    "# Convert predictions to binary classes based on the threshold\n",
    "y_pred_binary = [1 if val >= threshold else 0 for val in y_pred]\n",
    "\n",
    "# Print the binary predictions\n",
    "print(\"Binary Predictions:\", y_pred_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74955d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14df17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example for validation (optional)\n",
    "example_index = 208\n",
    "example_features = X.iloc[example_index]\n",
    "example_target = y[example_index]\n",
    "\n",
    "# Make predictions on the example\n",
    "example_prediction = model.predict([example_features])\n",
    "\n",
    "# Compare with actual value\n",
    "print(\"Example features:\", example_features)\n",
    "print(\"Example target:\", example_target)\n",
    "print(\"Example prediction:\", example_prediction)\n",
    "\n",
    "# Calculate mean squared error on the test set (optional)\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8242f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = y_pred_binary.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d3e9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
